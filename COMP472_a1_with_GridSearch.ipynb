{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT ALL NECCESSARY MODULES AND PACKAGES\n",
    "\n",
    "# TO PLOT DISTRIBUTIONS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NUMPY\n",
    "import numpy as np\n",
    "\n",
    "# ML MODELS\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# ML TEXT HELPERS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# ML METRICS\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION FILTERS AND RETURNS ALL THE REVIEWS' DOCS AND LABELS\n",
    "def read_documents(fileName):\n",
    "    all_docs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with open(fileName, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split(\" \", 3)\n",
    "            all_docs.append(words[3])\n",
    "            all_labels.append(words[1])\n",
    "\n",
    "    return all_docs, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVE ALL THE DATA FROM THE PASSED TEXT FILE\n",
    "textFileData = 'all_sentiment_shuffled.txt'\n",
    "all_docs, all_labels = read_documents(textFileData)\n",
    "\n",
    "# FROM THE RETRIEVED DATA, SPLIT THE DATA INTO TRAINING AND EVALUTATION SETS\n",
    "split_point = int(0.80*len(all_docs))\n",
    "train_docs = all_docs[:split_point]\n",
    "train_labels = all_labels[:split_point]\n",
    "eval_docs = all_docs[split_point:]\n",
    "eval_labels = all_labels[split_point:]\n",
    "\n",
    "# FIND ALL UNIQUE LABELS\n",
    "targetTypes = sorted(list(set(all_labels)))\n",
    "\n",
    "# CREATE A DICTIONARY MAPPING LABELS TO THEIR INDEX\n",
    "labelToIndexDict = dict()\n",
    "for i in range(len(targetTypes)):\n",
    "    labelToIndexDict[targetTypes[i]] = i\n",
    "\n",
    "# CONVERT LABELS INTO INDICES\n",
    "def getTargets(labels):\n",
    "    targets = []\n",
    "    for sentiment in labels:\n",
    "        targets.append(labelToIndexDict[sentiment])\n",
    "    return targets\n",
    "\n",
    "# RETRIEVE CONVERSION FOR TRAINING AND EVALUATION SETS\n",
    "trainTargets = getTargets(train_labels)\n",
    "evalTargets = getTargets(eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE DISTRIBUTION OF THE NUMBER OF THE INSTANCES IN EACH CLASS\n",
    "numberData = [all_labels.count(label) for label in targetTypes]\n",
    "barGraph = plt.bar(targetTypes,numberData)\n",
    "\n",
    "for i in range(0, len(barGraph), 2):\n",
    "    barGraph[i].set_color('r')\n",
    "\n",
    "plt.title('Number of instances in each class')\n",
    "\n",
    "# ADD COUNT FOR EACH CLASS TO THE GRAPH\n",
    "for i, v in enumerate(numberData):\n",
    "    plt.text(plt.xticks()[0][i] - 0.10, v + 50, str(v))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FOR NAIVE BAYES MODEL \n",
    "def trainNaiveBayes(trainDocs, trainTargets):\n",
    "    return MultinomialNB().fit(trainDocs, trainTargets)\n",
    "\n",
    "# TRAINING FOR BASE DT MODEL \n",
    "def trainBaseDT(trainDocs, trainTargets):\n",
    "    return DecisionTreeClassifier(criterion=\"entropy\").fit(trainDocs, trainTargets)\n",
    "\n",
    "# TRAINING FOR BEST DT MODEL \n",
    "def trainBestDT(trainDocs, trainTargets):\n",
    "    return DecisionTreeClassifier(criterion=\"gini\",\n",
    "                                  splitter=\"best\",\n",
    "                                  max_depth=None,\n",
    "                                  min_samples_split=4,\n",
    "                                  min_samples_leaf=1).fit(trainDocs, trainTargets)\n",
    "\n",
    "# GIVEN THE ML MODEL AND THE EVALUATION SET, MAKE THE PREDICTIONS\n",
    "def classify(clf,evalDocs):\n",
    "    return clf.predict(evalDocs)\n",
    "\n",
    "# PLOTS THE CONFUSION MATRIX      \n",
    "def plotConfusionMatrix(clf,prediction):\n",
    "    plot_confusion_matrix(clf, eval_docs_tfidf, evalTargets, labels=[i for i in range(len(targetTypes))])\n",
    "    return confusion_matrix(evalTargets, prediction, labels=[i for i in range(len(targetTypes))])\n",
    "        \n",
    "# RETRIEVE OUTPUT CLASSIFICATION METRICS\n",
    "def metrics(predicted):\n",
    "    precision,recall,f1,support = precision_recall_fscore_support(evalTargets,\n",
    "                                                                  predicted,\n",
    "                                                                  labels=[i for i in range(len(targetTypes))],\n",
    "                                                                  zero_division=0\n",
    "                                                                 )\n",
    "    return precision,recall,f1\n",
    "\n",
    "# MEASURES THE PREDICTION ACCURACY OF THE MODEL\n",
    "def accuracy(predicted):\n",
    "    return accuracy_score(evalTargets, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT EACH WORD OF THE VOCABULARY FOUND IN THE TRAINING \n",
    "# TO A CORRESPONDING INDEX AND COUNT EACH WORD\n",
    "# NOTE: TO SEE THE VOCABULARY OF THE TRAINING SET: print(countVect.get_feature_names())\n",
    "# NOTE: TO OUTPUT THE INDEX OF A WORD IN THE VOCAB (from countVect.get_feature_names()): countVect.vocabulary_.get('WORD') \n",
    "# UNDERSTANDING COUNTVECTORIZER()\n",
    "# EX |(0, 23)     1|\n",
    "# 0 is the index corresponding to the review, so 0 is the first review about a bad album\n",
    "# 23 corresponds to the word at index 23 in countVect.get_feature_names()\n",
    "# 1 is the number of times word 23 shows up in review 0\n",
    "countVect = CountVectorizer()\n",
    "\n",
    "### PREPARING TRAINING SET\n",
    "train_docs_counts = countVect.fit_transform(train_docs)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_docs_tfidf = tfidf_transformer.fit_transform(train_docs_counts)\n",
    "# TRAINING DONE\n",
    "\n",
    "### PREPARING EVALUATION SET\n",
    "eval_docs_counts = countVect.transform(eval_docs)\n",
    "eval_docs_tfidf = tfidf_transformer.transform(eval_docs_counts)\n",
    "# EVALUATION SET DONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVES BAYES CLASSIFIER\n",
    "nb = trainNaiveBayes(train_docs_tfidf, trainTargets)\n",
    "nbPrediction = classify(nb,eval_docs_tfidf)\n",
    "nbMetrics=[]\n",
    "\n",
    "# CONFUSION MATRIX\n",
    "nbMetrics.append(plotConfusionMatrix(nb,nbPrediction))\n",
    "\n",
    "# RETRIEVE METRICS\n",
    "nbPrecision, nbRecall, nbF1_measure = metrics(nbPrediction)\n",
    "nbMetrics.append(nbPrecision)\n",
    "nbMetrics.append(nbRecall)\n",
    "nbMetrics.append(nbF1_measure)\n",
    "\n",
    "# RETRIEVE ACCURACY OF PREDICTIONS\n",
    "nbMetrics.append(accuracy(nbPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE DT CLASSIFIER\n",
    "baseDT = trainBaseDT(train_docs_tfidf, trainTargets)\n",
    "baseDTPrediction = classify(baseDT,eval_docs_tfidf)\n",
    "baseDTMetrics=[]\n",
    "\n",
    "# CONFUSION MATRIX\n",
    "baseDTMetrics.append(plotConfusionMatrix(baseDT,baseDTPrediction))\n",
    "\n",
    "# RETRIEVE METRICS\n",
    "baseDTPrecision, baseDTRecall, baseDTF1_measure = metrics(baseDTPrediction)\n",
    "baseDTMetrics.append(baseDTPrecision)\n",
    "baseDTMetrics.append(baseDTRecall)\n",
    "baseDTMetrics.append(baseDTF1_measure)\n",
    "\n",
    "# RETRIEVE ACCURACY OF PREDICTIONS\n",
    "baseDTMetrics.append(accuracy(baseDTPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST DT CLASSIFIER\n",
    "bestDT = trainBestDT(train_docs_tfidf, trainTargets)\n",
    "bestDTPrediction = classify(bestDT,eval_docs_tfidf)\n",
    "bestDTMetrics=[]\n",
    "\n",
    "# CONFUSION MATRIX\n",
    "bestDTMetrics.append(plotConfusionMatrix(bestDT,bestDTPrediction))\n",
    "\n",
    "# RETRIEVE METRICS\n",
    "bestDTPrecision, bestDTRecall, bestDTF1_measure = metrics(bestDTPrediction)\n",
    "bestDTMetrics.append(bestDTPrecision)\n",
    "bestDTMetrics.append(bestDTRecall)\n",
    "bestDTMetrics.append(bestDTF1_measure)\n",
    "\n",
    "# RETRIEVE ACCURACY OF PREDICTIONS\n",
    "bestDTMetrics.append(accuracy(bestDTPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def modelToFile(modelType,modelMetrics,prediction):\n",
    "    \n",
    "    #CREATE OUTPUT FILE\n",
    "    file = open(modelType+\"-\"+textFileData,\"w\")\n",
    "    \n",
    "    file.write(\"LEGEND\\n-----------------------\\n\")\n",
    "    for index in range(len(targetTypes)):\n",
    "        file.write(\"%s: %s\\n\" % (index, targetTypes[index]))\n",
    "    file.write(\"\\n\\n\")\n",
    "    \n",
    "    # WRITE CONFUSION MATRIX\n",
    "    file.write(\"CONFUSION MATRIX\\n\")\n",
    "    file.write(\"(row is true label, column is predicted label)\\n-----------------------\\n\")\n",
    "    file.write(np.array2string(modelMetrics[0]))\n",
    "    file.write(\"\\n\\n\\n\")\n",
    "    \n",
    "    # WRITE PRECISION VALUES\n",
    "    file.write(\"PRECISION VALUES\\n-----------------------\\n\")\n",
    "    for index in range(len(targetTypes)):\n",
    "        file.write(targetTypes[index] +\": \" + str(modelMetrics[1][index].item()) +\"\\n\")\n",
    "    file.write(\"\\n\\n\")\n",
    "    \n",
    "    # WRITE RECALL VALUES\n",
    "    file.write(\"RECALL VALUES\\n-----------------------\\n\")\n",
    "    for index in range(len(targetTypes)):\n",
    "        file.write(targetTypes[index] +\": \" + str(modelMetrics[2][index].item()) +\"\\n\")\n",
    "    file.write(\"\\n\\n\")\n",
    "    \n",
    "    # WRITE F1-MEASURE VALUES\n",
    "    file.write(\"F1-MEASURE VALUES\\n-----------------------\\n\")\n",
    "    for index in range(len(targetTypes)):\n",
    "        file.write(targetTypes[index] +\": \" + str(modelMetrics[3][index].item()) +\"\\n\")\n",
    "    file.write(\"\\n\\n\")\n",
    "    \n",
    "    # WRITE ACCURACY\n",
    "    file.write(\"PREDICTION ACCURACY\\n-----------------------\\n\")\n",
    "    file.write(\"The model's accuracy is: \" + str(modelMetrics[4].item()) +\"\\n\")\n",
    "    file.write(\"\\n\\n\")\n",
    "    \n",
    "    # WRITE REVIEW LINE NUMBER AND THE PREDICTION OF THE MODEL\n",
    "    file.write(\"BELOW ARE ALL THE PREDICTIONS MADE FOR EACH INSTANCES IN THE EVALUATION SET\\n(0-indexed)\\n--------------\\n\")\n",
    "    index = split_point;\n",
    "    for pred in prediction:\n",
    "        file.write('%i, %s%s\\n' % (index, pred, ' [Misclassified]' if evalTargets[index-split_point] != pred else ''))\n",
    "        index+=1\n",
    "         \n",
    "    file.close()\n",
    "    \n",
    "# OUTPUT TO FILES \n",
    "modelToFile(\"NaiveBayesClassifier\",nbMetrics,nbPrediction)\n",
    "modelToFile(\"BaseDT\",baseDTMetrics,baseDTPrediction)\n",
    "modelToFile(\"BestDT\",bestDTMetrics,bestDTPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND BEST PARAMETERS USING GRIDSEARCH FOR BEST_DT\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(steps=[('dec_tree', dec_tree)])\n",
    "\n",
    "X = train_docs_tfidf\n",
    "y = trainTargets\n",
    "\n",
    "parameters = {\n",
    "    'dec_tree__criterion': ['gini', 'entropy'],\n",
    "    'dec_tree__max_depth': [2,8,16,None],\n",
    "    'dec_tree__splitter': ['random', 'best'],\n",
    "    'dec_tree__min_samples_split': [2,4,8,16],\n",
    "    'dec_tree__min_samples_leaf': [1,2,4,8],\n",
    "}\n",
    "\n",
    "clf_GS = GridSearchCV(pipe, parameters, n_jobs=-1)\n",
    "clf_GS.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT RESULTS OF GRIDSEARCH\n",
    "\n",
    "print('Best criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\n",
    "print('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\n",
    "print('Best splitter:', clf_GS.best_estimator_.get_params()['dec_tree__splitter'])\n",
    "print('Best min_samples_split:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_split'])\n",
    "print('Best min_samples_leaf:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_leaf'])\n",
    "print();\n",
    "print(clf_GS.best_estimator_.get_params()['dec_tree'])\n",
    "print('Best estimator accuracy:', np.mean(accuracy(clf_GS.best_estimator_.predict(eval_docs_tfidf))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
